---
title: 应对 LLM 的局限性
description: 介绍如何应对这些局限性。
---

# **如何应对这些局限性**

虽然 LLM 存在上述局限性，但这并不意味着我们无法有效利用它们。通过采取一些明智的策略，并结合前面学到的提示词技巧，我们可以在很大程度上规避风险，更安全、更有效地使用 LLM：

* **交叉验证与事实核查 (Cross-validation and Fact-checking)**: 这是应对“幻觉”和知识不准确最重要的一道防线。对于 LLM 提供的任何关键信息、数据、事实性陈述，尤其是在用于学术研究、商业决策或任何重要场合之前，**务必通过其他可靠的、独立的信源进行核实**。这些信源可以包括权威的搜索引擎、专业的学术数据库、官方出版物、领域专家等。切记，不要盲目相信 LLM 输出的一切内容。
* **通过清晰指令和充足上下文减少歧义**: 正如在提示词工程部分所强调的，在提问时尽可能清晰、具体地表述您的需求，并提供充足的背景信息。这有助于减少模型因误解您的意图而产生不相关回答或“幻觉”的可能性 。
* **注意提问方式，有策略地引导输出**:
  * **应对偏见**: 当讨论可能涉及潜在偏见的主题时（如不同群体的特征、社会问题等），您可以尝试要求模型从多个不同角度进行分析，或者在提示中明确指示其保持中立和客观，避免使用刻板印象。例如，可以加入这样的指令：“请确保你的回答是中立和客观的，不依赖于任何刻板印象或概括性的描述。” 。
  * **应对知识截止**: 在提问涉及近期事件或需要最新数据的问题时，要时刻意识到模型可能存在的知识截止日期。如果模型无法提供最新信息，应考虑结合搜索引擎等其他工具来获取。提问时，也可以主动限定知识范围，例如：“根据你截止到[模型知识截止日期]的知识，请回答……”
  * **应对有限的推理能力**: 对于需要复杂逻辑或计算的问题，可以使用思维链提示（CoT）来引导模型逐步思考，并仔细检查其给出的推理过程和每一步的结果，而不是仅仅关注最终答案。
* **利用约束条件限定信息来源**: 如果您希望模型仅基于您提供的特定文本内容来回答问题或进行总结（而不是从其庞大的通用知识库中提取信息），可以在提示中明确设定这一约束。这有助于减少模型引入外部不准确或过时信息的风险 。
  * *示例*：“请仔细阅读以下提供的关于‘可持续农业实践’的文章节选，并且**仅根据这篇文章的内容**回答接下来的问题。不要使用文章之外的任何信息或你自己的知识。文章节选如下：[在此粘贴文章内容] 问题是：……”
* **保持批判性思维和人类判断**: 始终对 LLM 生成的内容保持一种健康的怀疑态度和批判性眼光。要理解 LLM 本质上是一个强大的辅助工具，它可以提供信息、启发思路、提高效率，但它不能替代人类的深度思考、创造力、伦理判断和最终决策责任 。
* **通过迭代和调整优化输出**: 如果第一次得到的输出不理想，或者发现其中包含错误、偏见等问题，不要轻易放弃。尝试修改您的提示词，从不同的角度重新提问，或者将复杂的问题分解成更小的部分逐一攻克。
* **了解您所使用的具体模型**: 不同的 LLM在能力、特性、知识截止日期、以及对特定类型提示的响应方式上可能会有所不同。如果条件允许，花一些时间了解您正在使用的特定模型的特点和已知局限性，有助于您更有效地与之互动。

值得强调的是，**学习如何应对 LLM 的局限性，与提升您的提示词工程技巧是相辅相成、紧密关联的**。例如：

* 当您了解到 LLM 可能会产生“幻觉”时，您就会更倾向于在提示中要求模型“仅基于提供的上下文回答”，这本身就是一种高级的提示技巧。
* 当您意识到模型可能存在偏见时，您可能会在提示中加入要求“从多个视角分析”或“保持中立”的指令。
* 当您知道模型有知识截止日期时，您会在提问时更注意问题的时效性，或者主动在提示中提供最新的背景信息。
* 当您认识到模型推理能力有限时，您会更有意识地运用思维链提示等技巧来辅助其思考。

因此，掌握提示词工程的各项原则和技巧，不仅能帮助您更好地发挥 LLM 的优势，也能让您更从容地规避其潜在的风险。这种理解和技能的结合，将使您在与 LLM 的协作中更加得心应手。