---
title: LLM 的局限性
description: 介绍LLM 常见的局限性。
---

# **LLM 并非万能：简述常见局限性**

* **“幻觉” (Hallucinations) / 编造信息**: 这是 LLM 最为人所知的局限性之一。所谓“幻觉”，是指模型有时会生成一些听起来非常逼真、细节丰富，但实际上完全错误或凭空捏造的信息。这种情况尤其容易发生在模型对其知识库中不包含的信息进行推测，或者当用户提出的问题超出其训练数据范围时。它们可能会自信地混合事实与虚构内容，或者煞有介事地陈述一个根本不存在的“事实”。
  * *例如*：如果您询问一个非常冷僻或不存在的历史事件的细节，LLM 可能会“创造”出相关的日期、人物和情节。
* **偏见 (Bias)**: LLM 的“知识”来源于其训练数据，而这些数据绝大部分来自互联网上由人类生成的文本。互联网本身就充满了各种显性或隐性的社会偏见，如性别歧视、种族刻板印象、文化偏见等。因此，LLM 在学习过程中，不可避免地会习得并可能在输出中无意识地反映甚至放大这些偏见。
  * *例如*：当被要求描述某些职业的典型从业者时，模型的回答可能不自觉地偏向某一性别或特定人群。
* **知识截止 (Limited Knowledge / Knowledge Cut-off)**: 大多数 LLM 的知识库并非实时更新。它们的知识通常仅限于其训练数据收集完成的那个特定时间点（即“知识截止日期”）。对于发生在该日期之后的事件、发现或信息，模型通常是不知道的，除非它被特别设计为可以连接互联网进行实时查询。
  * *例如*：如果您向一个知识截止于 2023 年初的模型询问 2024 年发生的重大新闻，它很可能无法提供准确信息，甚至可能基于过时信息进行不当推测。
* **有限的推理能力 (Limited Reasoning Skills)**: 尽管 LLM 在处理语言相关的任务（如写作、总结、翻译）上表现出色，但在需要进行复杂逻辑推理、多步骤数学运算、精细的常识判断或严格的因果分析时，它们的能力仍然有限，容易出错。它们主要依赖于从数据中学习到的模式匹配和统计关联，而非像人类那样进行真正的逻辑推导和深度理解。
  * *例如*：对于一个包含多个变量和隐含条件的复杂数学应用题，或者需要洞察深层社会现象背后原因的问题，LLM 的回答可能不够可靠。
* **缺乏真正的理解和常识**: 正如之前多次强调的，LLM 是基于统计规律来生成文本的，它们并不具备人类所拥有的真实理解力、情感、意图或丰富的常识知识体系。它们的回应是基于模式的“模仿”，而非基于对世界真实运作方式的深刻认知。
* **上下文长度限制 (Contextual limitations / Token limits)**: LLM 在一次交互中能够处理和“记住”的文本量（即上下文窗口）是有限的。这个限制通常用“词元” (tokens) 的数量来衡量。当对话非常长，或者需要模型处理的文档非常庞大时，模型可能会“忘记”对话早期的内容，或者难以在整个长文本中保持主题和逻辑的一贯性。
* **对提示词的敏感性 (Prompt Sensitivity / Potential for "Hacking")**: LLM 的输出结果对其接收到的提示词中的细微变化非常敏感。有时，仅仅改变几个词或调整一下语序，就可能导致输出结果的巨大差异。更有甚者，一些了解模型特性的用户可能会通过精心设计的、有时是带有欺骗性的提示词（即“提示词攻击”或“越狱提示”），来诱导模型绕过其安全设置，生成一些不当、有害或违反其使用规则的内容。

理解这些局限性的来源至关重要。它们并非是 LLM 设计中的偶然缺陷或简单的“bug”，而是在很大程度上根植于当前 LLM 技术范式的本质。例如：

* LLM 从数据中学习统计模式而非绝对真理，这使其在数据不完整或模式具有误导性时容易产生“幻觉”。
* LLM 的训练数据源自充满人类偏见的互联网，这使其不可避免地会继承这些偏见。
* LLM 的训练通常是一个阶段性的过程，其知识库在训练完成后就被“冻结”在某个时间点，从而导致了知识截止的问题。
* LLM 作为复杂的模式匹配器而非真正的逻辑推理引擎，这决定了它们在需要深度推理的任务上表现有限。
* 当前主流的 Transformer 等模型架构本身对能够处理的输入序列长度就存在固有的技术限制，导致了上下文窗口的局限。

认识到这些局限性是 LLM 技术本质的反映，能帮助我们建立更现实的期望，避免将其神化。同时，这也强调了在使用 LLM 时，人类的监督、批判性思维和最终判断仍然是不可或缺的。