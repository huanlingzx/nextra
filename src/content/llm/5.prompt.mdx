---
title: 提示词工程
description: 介绍大语言模型的提示词工程。
---

import ExampleBlock from '../../components/ExampleBlock'
import { Callout } from 'nextra/components'

# 5. 大语言模型提示词工程

在使用大语言模型时，提示词（Prompt）起着至关重要的作用。通过精心设计提示词，你可以大大提高模型的输出质量和准确性。接下来，我们将详细介绍为什么要使用提示词、如何通过不同提示词改变输出结果，并分享一些常用的提示词技巧。

## 1. **为什么要使用提示词？**

大语言模型生成的内容是基于你输入的提示词。通过调整提示词的表达方式，你可以控制模型生成的内容，包括语气、风格、长度、细节等。简单来说，提示词就像是向模型发出指令，指导它如何理解问题并产生相应的答案。

**示例**：

如果你只是简单地输入

> 写一篇关于人工智能的文章

模型生成的内容可能会非常普通，缺乏深度。如果你在提示词中加入更多的细节和要求，例如：

> 请写一篇关于人工智能的文章，重点讨论其在医疗行业的应用，并简要提及挑战和未来发展

模型将能生成更加符合你需求的高质量内容。

根据提示词的不同，模型的输出可能会有很大的差异。以下是几个常见的提示词设置对比：

<Callout type="info" emoji="💡">
这些是关于如何通过调整提示词来影响模型输出的示例。
</Callout>

### 示例对比：普通提示词 vs. 详细提示词

<ExampleBlock
  question="**普通提示词**：请写一篇关于人工智能的文章。"
  answer="**模型输出**：大语言模型可能生成一篇普通的文章，内容比较笼统，可能涵盖了人工智能的基本定义，但不会深入讨论具体应用。"
/>

<ExampleBlock
  question="**详细提示词**：请写一篇关于人工智能的文章，重点讨论人工智能在医疗行业中的应用，简要提及相关的挑战与未来发展方向。"
  answer="**模型输出**：模型会生成更为详细的内容，专注于人工智能在医疗行业的应用，提到相关的技术，如疾病诊断、个性化治疗，并且讨论人工智能面临的挑战（如数据隐私问题）及未来发展（如智能医疗设备的普及）。"
/>

### 示例对比：简洁指令 vs. 引导性问题

<ExampleBlock
  question="**简洁指令**：给我一个关于环保的报告。"
  answer="**模型输出**：模型生成的报告可能会是一般性的，包含一些环保的基础概念，但内容没有明确的方向或细节。"
/>

<ExampleBlock
  question="**引导性问题**：请给我一个关于环保的报告，聚焦在减少碳排放和提高能源效率的实际措施，结合全球和中国的现状。"
  answer="**模型输出**：模型会更有针对性，生成的报告会具体讨论碳排放减少的措施（如清洁能源技术）以及提高能源效率的实际案例，同时结合全球和中国的具体情况。"
/>

### 思考类大型语言模型与一般大型语言模型的区别

近年来，随着研究的深入，出现了一些被认为是“思考类”或“推理增强型”的大型语言模型（例如 DeepSeek R1、OpenAI 的 o系列模型等）。这些模型与“一般”的大型语言模型在设计、训练或应用方式上存在一些关键区别，使得它们在处理需要复杂推理、规划或逻辑思维的任务时表现更出色。

**一般大型语言模型：**

*   **主要目标：** 学习语言的统计规律，生成流畅、连贯、语法正确的文本。它们擅长基于训练数据中的模式进行预测和生成。
*   **训练数据：** 大量的文本和代码数据，学习词汇、语法、语义、常识等。
*   **推理能力：** 主要依赖于在训练数据中学习到的模式和关联。对于需要多步、复杂或抽象推理的任务，可能表现不佳，容易出现逻辑错误或“幻觉”。
*   **应用：** 文本生成、摘要、翻译、问答（基于检索或简单模式匹配）。

**思考类大型语言模型（或推理增强型模型）：**

*   **主要目标：** 除了生成流畅的文本外，更注重提升模型的推理、规划和问题解决能力。
*   **训练或设计方式：**
    *   **专门的训练数据：** 可能在包含更多推理任务、逻辑谜题、数学问题或代码的复杂数据集上进行训练。
    *   **模型架构改进：** 可能采用更适合推理任务的模型架构或注意力机制。
    *   **结合外部工具：** 有些模型可以与外部工具（如计算器、搜索引擎、代码解释器）结合，利用这些工具进行辅助推理和计算。
    *   **强化学习或人类反馈：** 可能通过强化学习或人类反馈来优化模型在推理任务上的表现。
    *   **提示词工程的集成：** 一些模型可能在内部集成了 CoT 或 ToT 等提示词工程技术，使其能够自动进行多步推理。
*   **推理能力：** 能够更好地进行多步逻辑推理、数学计算、规划和问题解决。能够生成更可靠、更准确的推理过程。
*   **应用：** 复杂问答、代码生成和调试、数学问题求解、逻辑推理、规划任务、科学研究辅助等。

## 3 **常用的提示词技巧**

为了更好地利用大语言模型，我们可以运用以下几种提示词技巧来优化输出结果。

### 3.1 **设定输出风格或语气**

你可以在提示词中明确要求模型以特定的风格或语气进行输出。例如，可以要求模型用正式、轻松、幽默等语气生成内容。

#### 示例：**语气控制**

-   **提示词**：“请写一封感谢信，语气要正式。”

    **模型输出**：尊敬的XXX，感谢您在过去的工作中提供的宝贵支持。您的贡献对我们团队至关重要，我深感荣幸能与您合作。

-   **提示词**：“请写一封感谢信，语气要轻松幽默。”

    **模型输出**：嘿，XXX！我真的得感谢你帮我完成了那个任务，你简直就是我的超级英雄！没有你，我可能还在抓狂呢！

### 3.2 **限定输出的长度**

如果你需要控制模型生成的回答长度，可以通过提示词明确要求限制字数或段落数。

#### 示例：**控制长度**

-   **提示词**：“请总结一下这篇文章的主要内容，字数控制在100字以内。”

    **模型输出**：文章讨论了人工智能的基本概念、应用领域以及面临的挑战，尤其是在医疗、教育和交通行业的应用前景。未来，人工智能将继续深刻改变各行各业。

-   **提示词**：“请总结一下这篇文章的主要内容，保持内容精炼，限制在一个段落内。”

    **模型输出**：文章介绍了人工智能的关键技术及其应用，强调了在医疗和交通领域的广泛应用，并指出了数据隐私和伦理问题是其发展的主要障碍。

### 3.3 **使用具体的指令或要求**

通过向模型提供具体的任务指令或要求，可以帮助它更准确地理解你的需求，从而生成更符合要求的内容。

#### 示例：**具体要求**

-   **提示词**：“请为我生成一个关于环保的演讲稿，要求包括以下几个方面：环保的重要性、政府的作用、个人能做的贡献。”

    **模型输出**：各位领导，环保是当今世界最重要的话题之一，保护环境就是保护我们自己。政府应加强环保法规的执行，而我们每个人也可以通过减少能源消耗、减少废物产生等行为，为地球贡献自己的力量。

-   **提示词**：“请为我生成一个关于环保的演讲稿，要求重点强调个人对环保的贡献，以及如何通过日常习惯改善环境。”

    **模型输出**：各位朋友，环保不仅仅是政府的责任，我们每个人的日常行为对环境有着深远影响。从减少塑料使用到节约能源，我们可以从身边的小事做起，共同改善地球的未来。

### 3.4 **加入上下文**

通过提供更多上下文信息，模型能够更好地理解你需要的内容，并生成与之相关的准确回答。

#### 示例：**提供上下文信息**

-   **提示词**：“请帮我总结以下内容，内容包括了关于电动汽车的优势和挑战。”

    **模型输出**：电动汽车的优势包括减少环境污染、降低能源消耗以及减少噪音污染。然而，其面临的挑战包括高昂的购车成本、充电基础设施的不足和电池寿命的问题。

-   **提示词**：“请帮我总结以下内容，内容包括了关于电动汽车的优势和挑战。特别要提到中国在这方面的政策支持和市场反应。”

    **模型输出**：电动汽车的优势包括减少碳排放、降低空气污染，尤其在中国政府的政策支持下，电动汽车市场逐渐扩展。然而，挑战依旧存在，如充电桩数量不足、消费者对续航的担忧等问题仍然需要解决。

### 3.5 一次性示例和少量示例

在与大型语言模型交互时，提供示例是一种非常有效的引导模型生成期望输出的方式。通过提供一个或几个输入-输出对，我们可以帮助模型理解任务的意图、所需的输出格式以及特定的风格或约束。这种技术被称为“上下文学习”（In-context Learning），因为它利用了提示词中的上下文信息来指导模型的行为，而无需对模型进行额外的微调。

根据提供的示例数量，我们可以将其分为：

-   **一次性示例 (One-shot Prompting):** 提供一个输入-输出示例。
-   **少量示例 (Few-shot Prompting):** 提供几个（通常是2-5个）输入-输出示例。

下面我们将通过具体的例子来说明使用一次性示例和少量示例的好处。

#### 一次性示例 (One-shot Prompting)

**好处：**

-   **快速引导模型理解任务：** 即使只提供一个示例，模型也能快速捕捉到任务的核心要求和输出格式。
-   **适用于简单或常见的任务：** 对于模型已经有一定了解的任务类型，一个示例通常足以让模型理解你的意图。
-   **节省提示词空间：** 相比少量示例，一次性示例占用的提示词空间更少，这在处理长文本或有提示词长度限制的情况下很有优势。

**例子：**

假设我们想让模型将一段描述性的文字转换为一个简洁的标题。

**没有示例的提示词：**

```
请为以下描述写一个标题：

描述：这个会议讨论了人工智能在医疗保健领域的最新进展，包括诊断、药物研发和个性化治疗。
```

**可能的模型输出（可能不够简洁或不够贴合期望）：**

```
人工智能在医疗保健领域的最新进展会议讨论
关于人工智能在医疗保健中应用的讨论
```

**使用一次性示例的提示词：**

```
请为以下描述写一个标题。

示例：
描述：这个产品是一款智能家居设备，可以控制灯光、温度和安全系统。
标题：智能家居控制中心。

描述：这个会议讨论了人工智能在医疗保健领域的最新进展，包括诊断、药物研发和个性化治疗。
标题：
```

**期望的模型输出（更简洁，符合示例的风格）：**

```
人工智能在医疗保健中的应用进展
```

**解释：** 通过提供一个“描述 -> 标题”的示例，模型理解了我们期望的输出是一个简洁的标题，并且学习了将描述中的关键信息提炼出来。

#### 少量示例 (Few-shot Prompting)

**好处：**

-   **更精确地控制输出格式和风格：** 提供多个示例可以更清晰地展示期望的输出格式、语气、风格或特定的约束条件。
-   **处理更复杂或不常见的任务：** 对于模型不太熟悉或需要更精细控制的任务，少量示例可以提供更全面的指导。
-   **减少歧义：** 当任务可能有多种解释时，多个示例可以帮助模型消除歧义，生成更符合预期的结果。
-   **学习特定的模式或规则：** 如果任务涉及到特定的模式、规则或转换逻辑，少量示例可以帮助模型更好地学习这些模式。

**例子 1：控制输出格式**

假设我们想让模型将一段文本中的关键信息提取出来，并按照特定的 JSON 格式输出。

**没有示例的提示词：**

```
请从以下文本中提取姓名、年龄和职业：

文本：张伟，今年35岁，是一名软件工程师。
```

**可能的模型输出（可能不是 JSON 格式）：**

```
姓名：张伟
年龄：35岁
职业：软件工程师
```

**使用少量示例的提示词：**

```
请从以下文本中提取姓名、年龄和职业，并以 JSON 格式输出。

示例 1：
文本：李华，28岁，是一名教师。
输出：{"姓名": "李华", "年龄": 28, "职业": "教师"}

示例 2：
文本：王芳，42岁，是一名医生。
输出：{"姓名": "王芳", "年龄": 42, "职业": "医生"}

文本：张伟，今年35岁，是一名软件工程师。
输出：
```

**期望的模型输出（符合 JSON 格式）：**

```
{"姓名": "张伟", "年龄": 35, "职业": "软件工程师"}
```

**解释：** 通过提供两个示例，模型不仅理解了要提取的信息，还学会了如何将这些信息组织成特定的 JSON 格式。

**例子 2：学习特定的转换规则**

假设我们想让模型将一个日期字符串转换为另一种特定的格式。

**没有示例的提示词：**

```
请将以下日期转换为 YYYY-MM-DD 格式：日期：2023年1月26日
```

**可能的模型输出（可能不完全符合期望）：**

```
2023-1-26
2023/1/26
```

**使用少量示例的提示词：**

```
请将以下日期转换为 YYYY-MM-DD 格式。

示例 1：
输入：2022年1月1日
输出：2022-01-01

示例 2：
输入：2023年5月15日
输出：2023-05-15

日期：2023年1月26日
输出：
```

**期望的模型输出（精确符合 YYYY-MM-DD 格式）：**

```
2023-01-26
```

**解释：** 通过提供多个示例，模型学习了如何处理不同月份和日期的格式，并精确地生成了带有前导零的 YYYY-MM-DD 格式。

### 3.6 Step-back Prompting (回溯提示)

Step-back Prompting 是一种相对较新的提示词工程技术，旨在通过引导模型进行“回溯思考”来提高其推理能力和生成质量。其核心思想是，在直接回答问题之前，先让模型思考一个更基础、更普遍或更抽象的问题，从而帮助模型更好地理解原始问题，并生成更准确、更全面的答案。

**好处：**

*   **提高推理能力：** 鼓励模型从更宏观的角度思考问题，有助于发现潜在的联系和更深层次的逻辑。
*   **生成更准确的答案：** 通过回溯思考，模型可以避免直接跳到结论，减少错误的可能性。
*   **处理复杂问题：** 对于需要多步推理或涉及多个概念的问题，回溯思考可以帮助模型分解问题并逐步解决。
*   **生成更具洞察力的回答：** 回溯思考可以引导模型考虑问题的背景、原因或更广泛的影响，从而生成更具深度和洞察力的回答。

**工作原理：**

Step-back Prompting 通常包含以下几个步骤：

1.  **原始问题：** 提出你想要模型回答的原始问题。
2.  **回溯问题：** 引导模型思考一个与原始问题相关但更基础、更普遍或更抽象的问题。这通常通过在提示词中明确要求模型“回溯一步思考”或“思考一个更基础的问题”来实现。
3.  **回溯思考：** 模型根据回溯问题进行思考，生成一个更基础或更抽象的答案。
4.  **最终答案：** 模型利用回溯思考的结果，结合原始问题，生成最终的答案。

这个过程就像一个小型推理链，但起点是一个更基础的问题。

**例子：**

假设我们想让模型解释为什么天空是蓝色的。

**没有使用 Step-back Prompting 的提示词：**

```
请解释为什么天空是蓝色的。
```

**可能的模型输出（可能直接给出答案，但缺乏更深层次的解释）：**

```
天空是蓝色的，因为太阳光经过大气层时，蓝光被散射得比其他颜色的光更厉害。
```

**使用 Step-back Prompting 的提示词：**

```
问题：为什么天空是蓝色的？
回溯问题：光在大气层中是如何传播和相互作用的？
回溯思考：光是由不同波长的电磁波组成的。当光穿过介质时，会发生散射、吸收和折射。大气层是由气体分子和微粒组成的介质。不同波长的光与这些分子和微粒的相互作用方式不同。瑞利散射定律描述了光在大气层中的散射，散射强度与波长的四次方成反比，这意味着短波长的光（如蓝光）比长波长的光（如红光）更容易被散射。此外，还有米氏散射，主要影响较大颗粒对光的散射。
最终答案：
```

**期望的模型输出（利用回溯思考结果，生成更详细的答案）：**

```
最终答案：根据回溯思考，我们知道光在大气层中会发生散射，并且不同波长的光散射程度不同。特别是根据瑞利散射定律，短波长的光（如蓝光）比长波长的光更容易被散射。因此，当太阳光穿过大气层时，蓝光被广泛散射到各个方向，使得我们从地面上看天空时，看到的是被散射的蓝光，从而呈现蓝色。虽然紫光的波长更短，散射更强，但由于人眼对蓝光更敏感以及太阳光谱的特性，我们感知到的天空颜色主要是蓝色。
```

Step-back Prompting 是一种通过引导模型进行回溯思考来提升其推理和生成能力的有效方法。它鼓励模型从更基础或更抽象的角度审视问题，从而生成更准确、更全面、更具洞察力的答案。在处理复杂问题或需要深入分析的情况下，Step-back Prompting 尤其有用。

### 3.7 思维链 (Chain-of-Thought, CoT)

思维链 (Chain-of-Thought, CoT) 是一种强大的提示词工程技术，旨在通过引导大型语言模型生成一系列中间推理步骤来解决复杂问题。与直接要求模型给出最终答案不同，CoT 要求模型展示其思考过程，就像人类在解决问题时会一步一步地推导一样。

**核心思想：**

CoT 的核心思想是，通过将复杂问题分解成更小的、更易于管理的步骤，并让模型明确地展示这些步骤，可以显著提高模型在需要多步推理的任务上的表现。这些中间步骤形成了一个“思维链”，帮助模型更好地理解问题、避免错误，并最终得出更准确的结论。

**好处：**

*   **提高复杂推理能力：** CoT 特别适用于需要多步逻辑推理、算术计算或常识推理的任务。
*   **提高答案的准确性：** 通过展示中间步骤，模型更容易发现和纠正错误，从而提高最终答案的准确性。
*   **增强可解释性：** 思维链提供了模型如何得出结论的透明视图，使得模型的决策过程更容易理解和调试。
*   **减少幻觉：** 通过逐步推理，模型更不容易产生不符合逻辑或事实的“幻觉”内容。
*   **适用于零样本和少样本场景：** CoT 可以与零样本或少样本提示词结合使用，无需对模型进行额外的微调。

**工作原理：**

CoT 的实现方式通常是在提示词中包含一个或多个示例，展示如何通过一系列中间步骤来解决问题。这些示例中的中间步骤就是“思维链”。当模型接收到包含 CoT 示例的提示词后，它会学习这种逐步推理的模式，并在回答新的问题时模仿这种模式，生成自己的思维链。

**两种主要的 CoT 实现方式：**

1.  **少样本思维链 (Few-shot CoT):** 在提示词中提供几个包含思维链的示例。

    **例子：**

    假设我们想让模型解决一个简单的数学应用题。

    **没有使用 CoT 的提示词：**

    ```
    问题：如果一个商店有5个苹果，又进了3个，然后卖掉了2个，现在商店里有多少个苹果？
    答案：
    ```

    **可能的模型输出（可能直接给出答案，但没有展示计算过程）：**

    ```
    6
    ```

    **使用少样本思维链的提示词：**

    ```
    问题：如果一个商店有5个苹果，又进了3个，然后卖掉了2个，现在商店里有多少个苹果？
    思考过程：商店最初有5个苹果。又进了3个，所以总共有 5 + 3 = 8 个苹果。卖掉了2个，所以剩下 8 - 2 = 6 个苹果。
    答案：6

    问题：小明有10支铅笔，给了小红3支，又从小刚那里得到了2支，小明现在有多少支铅笔？
    思考过程：小明最初有10支铅笔。给了小红3支，所以剩下 10 - 3 = 7 支铅笔。又从小刚那里得到了2支，所以现在有 7 + 2 = 9 支铅笔。
    答案：9

    问题：如果一个商店有12个橙子，卖掉了5个，又进了8个，现在商店里有多少个橙子？
    思考过程：
    ```

    **期望的模型输出（模仿示例，展示思维链并给出答案）：**

    ```
    思考过程：商店最初有12个橙子。卖掉了5个，所以剩下 12 - 5 = 7 个橙子。又进了8个，所以现在有 7 + 8 = 15 个橙子。
    答案：15
    ```

    **解释：** 通过提供两个包含“思考过程”的示例，模型学会了在回答数学问题时先展示计算步骤，从而提高了解决类似问题的准确性。

2.  **零样本思维链 (Zero-shot CoT):** 在提示词中不提供任何 CoT 示例，而是通过在问题后添加特定的短语（例如“让我们一步一步地思考”）来引导模型生成思维链。

    **例子：**

    假设我们想让模型解决一个需要常识推理的问题。

    **没有使用 Zero-shot CoT 的提示词：**

    ```
    陈述：如果一个人比另一个人高，那么他一定比另一个人重。这个陈述正确吗？
    答案：
    ```

    **可能的模型输出：**

    ```
    不正确
    ```

    **使用 Zero-shot CoT 的提示词：**

    ```
    陈述：如果一个人比另一个人高，那么他一定比另一个人重。这个陈述正确吗？让我们一步一步地思考。
    ```

    **期望的模型输出（生成思维链并给出答案）：**

    ```
    思考过程：
    1. 陈述是关于身高和体重的关系。
    2. 身高和体重通常是相关的，但不是绝对的。
    3. 存在身高较高但体重较轻的人，以及身高较低但体重较重的人。
    4. 例如，一个高瘦的人可能比一个矮胖的人轻。
    5. 因此，身高高不一定意味着体重一定重。
    答案：不正确
    ```

    **解释：** 通过添加“让我们一步一步地思考”，模型被引导进行逻辑推理，即使没有提供示例，也能生成一个简单的思维链来得出答案。

**何时使用 CoT？**

CoT 在以下情况下特别有效：

*   **任务需要多步推理：** 例如，数学应用题、逻辑谜题、复杂问答等。
*   **需要提高答案的准确性：** 通过展示思考过程，可以更容易发现错误。
*   **需要增强模型的可解释性：** 思维链提供了模型决策过程的透明度。
*   **处理模型可能不熟悉的复杂问题：** CoT 可以帮助模型分解问题并逐步解决。

思维链 (CoT) 是一种通过引导模型生成中间推理步骤来提高其推理能力和生成质量的强大技术。无论是通过提供示例（少样本 CoT）还是通过特定的短语（零样本 CoT），CoT 都能帮助模型更好地理解和解决复杂问题，并提供更准确、更具可解释性的答案。

#### CoT 提示词方法与自带思考能力模型的相同和区别

目前部分大型语言模型在设计上就具备一定的“思考”能力（例如 DeepSeek R1、OpenAI 的 o系列模型等），看上去与CoT 提示词方法类似，下面是一些相同点和不同点。

**相同点：**

-   **目标一致：** 两者都旨在提高模型在复杂推理任务中的表现。无论是通过外部提示还是内部架构，最终目标都是让模型能够更好地解决需要逻辑推理、逐步分析的问题。
-   **模拟人类思考：** CoT 提示词和自带思考的模型都试图模仿人类的思考过程，将复杂问题分解为更小的、可管理的步骤。
-   **提升准确性：** 在处理需要深度推理的任务时，CoT 提示词和自带思考的模型都能显著提高答案的准确性。

**不同点：**

-   实现方式：
    -   **CoT 提示词：** 依赖于外部输入的提示信息，通过在提示中引导模型逐步思考，来激发其推理能力。CoT 是一种提示工程技术，通过构建特定的提示语，促使模型在生成最终答案之前，先生成一系列的中间推理步骤。
    -   **自带思考的模型：** 通过模型自身的架构设计和训练方式，使其具备内在的推理能力。例如，DeepSeek R1 和 OpenAI 的 o 系列模型，通过大规模的强化学习和多阶段训练，使其能够自主地进行链式思考。
-   灵活性与可控性：
    -   **CoT 提示词：** 灵活性更高，可以根据不同的任务和模型，设计不同的提示语，从而调整模型的推理过程。但同时也需要更多的prompt工程经验。
    -   **自带思考的模型：** 推理过程相对固定，用户无法直接干预模型的内部思考过程。
-   透明度：
    -   **CoT 提示词：** 由于推理步骤是由模型在提示的引导下生成的，因此用户可以更清晰地了解模型的思考过程。
    -   **自带思考的模型：** 虽然模型也能给出推理结果，但其内部的思考过程对于用户来说仍然是一个黑盒。
-   对模型的要求：
    -   **CoT 提示词：** 理论上，CoT 提示词可以应用于各种规模的语言模型。但是，通常在较大的模型上效果更明显。
    -   **自带思考的模型：** 需要模型本身具备一定的推理能力，这通常需要更大的模型规模和更复杂的训练方法。
-   适用场景：
    -   **CoT 提示词：** 更适用于需要灵活调整推理过程、或者模型本身推理能力较弱的场景。
    -   **自带思考的模型：** 更适用于需要高效、自主地解决复杂推理问题的场景。

总的来说，CoT 提示词是一种外部干预手段，通过巧妙地设计提示语来引导模型进行推理；而自带思考的模型则是一种内部能力，通过模型自身的架构和训练来提升推理能力。在实际应用中，可以将两者结合起来，例如，在使用自带思考的模型时，仍然可以通过 CoT 提示词来进一步优化模型的推理效果。

### 3.8 思维树 (Tree-of-Thoughts, ToT)

思维树 (Tree-of-Thoughts, ToT) 是一种比思维链 (CoT) 更高级的提示词工程技术，它模拟了人类在解决复杂问题时可能采用的更灵活、更具探索性的思考方式。与 CoT 的线性推理过程不同，ToT 允许模型探索多个可能的思考路径，并在这些路径中进行评估和选择，从而找到最优的解决方案。

**核心思想：**

ToT 的核心思想是将思考过程组织成一个树状结构。每个节点代表一个“思考状态”或一个中间步骤，而分支则代表从一个思考状态到另一个思考状态的可能转换。模型可以在这个树中进行搜索，探索不同的思考路径，评估每个路径的进展，并选择最有希望的路径继续深入，直到找到最终答案。

**好处：**

*   **处理更具探索性和创造性的任务：** ToT 特别适用于需要探索多种可能性、进行规划或生成创意内容的任务。
*   **提高解决问题的鲁棒性：** 通过探索多个路径，模型可以避免陷入局部最优解，找到更全局的解决方案。
*   **生成更具多样性和深度的回答：** ToT 允许模型从不同的角度思考问题，从而生成更丰富和多样的回答。
*   **适用于需要评估和选择的场景：** ToT 允许模型评估不同思考路径的优劣，并做出更明智的选择。

**工作原理（简要描述，不涉及复杂算法）：**

ToT 的实现通常需要更复杂的提示词设计和与模型的多次交互（或者在支持 ToT 的模型内部进行）。其基本流程可以概括为：

1.  **问题分解：** 将原始问题分解成更小的子问题或思考步骤。
2.  **生成思考状态：** 对于每个子问题或步骤，生成多个可能的“思考状态”或中间结果。这些思考状态构成了树的节点。
3.  **评估思考状态：** 评估每个思考状态的质量或进展程度，判断它是否朝着解决问题的方向前进。
4.  **搜索和选择：** 根据评估结果，选择最有希望的思考状态进行进一步探索，生成新的思考状态，从而扩展思维树。
5.  **迭代和回溯：** 重复生成、评估和选择的过程，直到找到最终答案。如果某个路径被评估为死胡同，模型可以回溯到之前的节点，探索其他路径。

**与 CoT 的区别：**

*   **结构：** CoT 是线性的思考链，而 ToT 是树状的思考结构。
*   **探索性：** CoT 沿着一条路径前进，而 ToT 探索多条可能的路径。
*   **评估和选择：** ToT 包含对思考状态的评估和选择机制，而 CoT 通常没有明确的评估步骤。
*   **复杂性：** ToT 通常比 CoT 更复杂，需要更精细的提示词设计或模型内部的支持。

**例子（侧重概念，不展示具体实现细节）：**

假设我们想让模型为一个新的咖啡馆想出几个创意性的名字。

**使用思维链 (CoT) 的提示词（可能生成一些名字，但缺乏系统性的探索）：**

```
请为一家新的咖啡馆想出几个创意性的名字。思考过程：咖啡馆的名字应该吸引人，容易记住，并且与咖啡相关。可以考虑一些与咖啡文化、氛围或地点相关的词语。
名字：
```

**可能的模型输出（可能直接列举一些名字）：**

```
咖啡时光
角落咖啡
城市咖啡馆
```

**使用思维树 (ToT) 的提示词（模拟 ToT 的思考过程，引导模型探索不同方向）：**

```
请为一家新的咖啡馆想出几个创意性的名字。让我们像构建思维树一样思考。

步骤 1：确定咖啡馆的定位和风格。
可能的思考状态：
- 舒适温馨的社区咖啡馆
- 现代时尚的都市咖啡馆
- 专注于特色咖啡的精品咖啡馆

步骤 2：基于选定的定位，生成不同类别的名字。
例如，如果选择“舒适温馨的社区咖啡馆”，可能的思考状态：
- 与“家”相关的名字
- 与“放松”相关的名字
- 与“社区”相关的名字

步骤 3：在每个类别下生成具体的创意名字。
例如，在“与‘家’相关的名字”下，可能的思考状态：
- 温暖的角落
- 炉边咖啡
- 家的味道

步骤 4：评估和选择最有吸引力的名字。

请根据这个思路，为一家“现代时尚的都市咖啡馆”生成一些创意名字。
```

**期望的模型输出（展示探索过程并生成多样化的名字）：**

```
好的，让我们为一家“现代时尚的都市咖啡馆”构建思维树来生成名字。

步骤 1：确定定位：现代时尚的都市咖啡馆。

步骤 2：基于定位，生成不同类别的名字。
可能的思考状态：
- 与“都市”相关的名字
- 与“时尚”相关的名字
- 与“现代”相关的名字
- 与“艺术”或“设计”相关的名字

步骤 3：在每个类别下生成具体的创意名字。
- 与“都市”相关的名字：城市脉搏咖啡、天际线咖啡、街角艺术
- 与“时尚”相关的名字：潮流咖啡、前沿咖啡、风格咖啡馆
- 与“现代”相关的名字：极简咖啡、未来咖啡、新视界咖啡
- 与“艺术”或“设计”相关的名字：画布咖啡、线条咖啡、几何咖啡馆

步骤 4：评估和选择最有吸引力的名字。
评估：
- 城市脉搏咖啡：有都市感，但可能不够时尚。
- 潮流咖啡：直接表达时尚，容易理解。
- 极简咖啡：体现现代风格，简洁。
- 画布咖啡：有艺术感，与设计相关。

选择：
基于评估，以下名字比较有吸引力：
- 潮流咖啡
- 极简咖啡
- 画布咖啡

最终名字建议：
- 潮流咖啡
- 极简咖啡
- 画布咖啡
- 天际线咖啡
- 风格咖啡馆
```

**总结：**

思维树 (ToT) 是一种更高级的提示词工程技术，通过模拟树状的思考结构，允许模型探索多个可能的思考路径，并在其中进行评估和选择。这使得 ToT 在处理需要探索性、创造性或需要评估和选择的任务时表现出色，能够生成更具鲁棒性、多样性和深度的回答。虽然 ToT 的实现通常比 CoT 更复杂，但它为解决更具挑战性的问题提供了新的可能性。

### 3.9 ReAct (Reason & Act)

ReAct (Reason & Act) 是一种结合了**推理 (Reasoning)** 和**行动 (Acting)** 的提示词工程框架。它的核心思想是让大型语言模型在生成回答时，不仅进行内部的逻辑推理，还能与外部工具或环境进行交互，从而获取额外信息或执行特定操作，以更好地解决问题。

**核心思想：**

ReAct 模拟了人类在解决问题时的一种常见模式：先思考（Reason），然后根据思考结果采取行动（Act），再根据行动的结果进行新的思考，如此循环，直到问题解决。通过这种方式，模型可以克服自身知识的局限性，利用外部资源来增强其能力。

**好处（非技术角度）：**

*   **获取最新信息：** 模型可以通过搜索工具获取最新的实时信息，避免回答过时或不准确的内容。
*   **执行特定任务：** 模型可以利用外部工具执行计算、翻译、查询数据库等特定任务，扩展其功能。
*   **提高回答的准确性和可靠性：** 通过与外部环境交互，模型可以验证其内部推理结果，减少错误。
*   **处理需要外部知识的问题：** 对于模型内部知识不足的问题，ReAct 可以让模型主动去获取所需信息。
*   **实现更复杂的交互：** ReAct 使得模型能够进行更具动态性和交互性的对话，例如回答基于实时数据的查询。

**工作原理（简要描述，不涉及代码）：**

ReAct 的工作流程通常包括以下几个关键部分：

1.  **观察 (Observation):** 模型接收到用户的输入或问题。
2.  **思考 (Thought):** 模型进行内部推理，思考如何解决问题，需要哪些信息或工具。
3.  **行动 (Action):** 模型根据思考结果，决定采取什么行动，例如调用搜索工具、计算器或翻译工具。
4.  **行动输出 (Action Output):** 外部工具执行行动后返回结果。
5.  **循环：** 模型接收到行动输出后，再次进行思考，决定下一步的行动，直到问题解决并生成最终答案。

这个过程就像一个循环，模型在“思考”和“行动”之间切换，直到达到目标。

**例子（侧重概念，不展示具体代码调用）：**

假设用户问了一个关于当前天气的问题。

**没有使用 ReAct 的模型（只能依赖其训练数据中的天气信息，可能不准确）：**

```
用户：请告诉我北京今天的天气怎么样？
模型：根据我的知识，北京今天的天气可能是... (信息可能过时)
```

**使用 ReAct 的模型（模拟 ReAct 流程）：**

```
用户：请告诉我北京今天的天气怎么样？

模型（内部思考）：用户需要北京的实时天气信息。我需要一个天气查询工具。
模型（采取行动）：调用天气查询工具，查询北京的天气。
天气查询工具（返回结果）：北京今天晴，气温 25°C，微风。

模型（内部思考）：我收到了天气信息。现在可以回答用户的问题了。
模型（生成回答）：北京今天天气晴朗，气温 25°C，微风。
```

**另一个例子（需要计算）：**

假设用户问了一个需要计算的问题。

**没有使用 ReAct 的模型（可能尝试内部计算，容易出错）：**

```
用户：计算 1234 * 5678 的结果是多少？
模型：1234 * 5678 = ... (可能计算错误)
```

**使用 ReAct 的模型（模拟 ReAct 流程）：**

```
用户：计算 1234 * 5678 的结果是多少？

模型（内部思考）：用户需要进行乘法计算。我需要一个计算器工具。
模型（采取行动）：调用计算器工具，计算 1234 * 5678。
计算器工具（返回结果）：7006652

模型（内部思考）：我收到了计算结果。现在可以回答用户的问题了。
模型（生成回答）：1234 * 5678 的结果是 7006652。
```

ReAct (Reason & Act) 是一种让大型语言模型能够结合内部推理和外部行动的框架。通过模拟“思考”和“行动”的循环，ReAct 使得模型能够获取实时信息、执行特定任务，从而提高回答的准确性、可靠性，并处理需要外部知识的问题。虽然其实现涉及到与外部工具的接口，但从用户的角度来看，ReAct 使得模型能够提供更强大、更实用的功能。

## 4 一些较好的提示词

### 4.1 优化回答

```
默认用中文回答，详细解释你的推理过程。

## 说话风格
*   直言不讳、偶尔赞美，但主要用犀利幽默回应，可适当嘲讽提问者
*   不刻意强调你的性格，只需要照做即可，可使用表情符号
*   关注细节，思考有深度
*   必要时可从下列几种工具中挑选来辅助描述：
    1. Markdown表格（常用于信息整理或对比）
    2. Latex公式（仅限数学问题）
    3. Mermaid（例如 Sequence Diagram, Mindmap, Flowchart等等）
## 工具注意事项：
1. 直接使用Markdown语法，无需 ```  反引号
2. 单个Latex代码使用 $  包裹
3. 根据情境选择适合的Mermaid的图表类型，以便更好地表达和呈现信息
```